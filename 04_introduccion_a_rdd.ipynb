{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![img/pythonista.png](img/pythonista.png)](https://www.pythonista.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a *Resilient Distributed Datasets (RDD)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder procesar de forma distribuida grandes volúmenes de datos, *Apache Spark* utiliza los *Resilient Distributed Datasets (RDD)*, los cuales son abstracciones de datos que pueden ser particionadas y distribuidas de forma consistente dentro del cluster por medio del *SparkContext* y también pueden ser operados de forma paralela.\n",
    "\n",
    "Un *RDD* es una colección de datos que no necesariamente debe de tener una estructura o esquema.\n",
    "\n",
    "Las caracterísitcas principales de los *RDD* son:\n",
    "\n",
    "* Permite realizar la adecuada ejecución de cargas de cómputo en memoria.\n",
    "* Permite la ejecución de operaciones de evaluación \"perezosa\" (*lazy evaluation*).\n",
    "* Son estructuras con tolerancia a fallos.\n",
    "* Son estructuras inmutables.\n",
    "* Cuenta con la capacidad de particionamiento de los datos en un clúster.\n",
    "* Garantizar la persistencia de los datos.\n",
    "* Permitir la realización de operaciones granulares.\n",
    "\n",
    "\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de un *RDD*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen dos formas de crear un *RDD*.\n",
    "\n",
    "* Utilizando el método [```pyspark.SparkContext.parallelize()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.parallelize.html) sobre una colección de *Python*.\n",
    "* Referenciando un [*dataset* externo](https://spark.apache.org/docs/latest/rdd-programming-guide.html#external-datasets) en un sistema de almacenamiento externo.\n",
    "    * [```SparkContext.textFile()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.textFile.html)\n",
    "    * [```SparkContext.wholeTextFiles()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.wholeTextFiles.html)\n",
    "    * [```SparkContext.pickleFile()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.pickleFile.html)\n",
    "    * [```SparkContext.sequenceFile()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.sequenceFile.html)\n",
    "    * [```SparkContext.binaryFiles()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.binaryFiles.html)\n",
    "    * [```SparkContext.binaryRecords()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.binaryRecords.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará una aplicación de *Spark*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Introduccion a RDD\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará un *RDD* llamado ```rdd_lista``` a partir de un objeto de tipo ```list``` de *Python*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_lista = spark.sparkContext.parallelize([1, 2, 3, [4, 5], 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd_lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda leerá el archivo de texto [```data/quijote.txt```](data/quijote.txt), el cual contiene el texto completo del libro \"El ingenioso hidalgo don Quijote de la Mancha\" de Miguel de Cervantes Saavedra el cual fue publicado por el [proyecto Gutenberg](https://www.gutenberg.org/files/2000/2000-0.txt) y creará el *RDD* con nombre ```rdd_texto```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_texto = spark.sparkContext.textFile('data/quijote.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones de *RDDs*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las operaciones son acciones que pueden ser asignadas como tareas de procesamiento de un clúster. Los *RRD* pueden realizar operaciones de dos tipos:\n",
    "\n",
    "* [**Transformaciones**](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations). Son operaciones perezosas, que dan por resultado un nuevo *RDD*. Una operación perezosa no se realiza hasta que el objeto resultante de dicha operación es utilizado. Algunas transformaciones son [```flatMap()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap.html), [```map()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html), [```reduceByKey()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey.html), [```filter()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html), [```sortByKey()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortByKey.html).\n",
    "* [**Acciones**](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions). Son operaciones que realizan cómputos a partir del *RDD* y regresan algún valor. Algunas acciones son [```count()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html), [```collect()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html), [```first()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.first.html), [```max()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.max.html), [```reduce()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html).\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-to-pyspark-rdd-operations/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos de acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las siguientes celdas utilizarán el método [```pyspark.RDD.count()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html), el cual regresa el número de elementos de un *RDD*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_lista.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_texto.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las siguientes celdas utilizarán el método [```pyspark.RDD.first()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.first.html), el cual regresa el primer elemento de un *RDD*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_lista.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_texto.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las siguientes celdas utilizarán el método [```pyspark.RDD.collect()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html), el cual regresará un objeto de tipo ```list``` de *Python* con cada elemento del *RDD*. En este caso, cada línea del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineas_quijote = rdd_texto.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lineas_quijote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineas_quijote[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos de transformaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará un nuevo *RDD* en el que el método [```pyspark.RDD.flatMap()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap.html) tratará a cada palabra del texto como un elemento al aplicar el método [```str.split()```](https://www.w3schools.com/python/ref_string_split.asp) de *Python* a cada línea del *RDD* ```rdd_texto```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_texto.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_texto.flatMap(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda creará un *RDD* similar al de la celda anterior y ejecutará la función [```pyspark.RDD.filter()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html), la cual a su vez creará otro *RDD* que excluya los textos vacíos. A este nuevo *RDD* se le dará el mombre de ```palabras_quijote```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_quijote = rdd_texto.flatMap(lambda x: x.split(\" \"))\\\n",
    "    .filter(lambda x: x != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_quijote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_quijote.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de conteo de palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siquiente celda creará el *RDD* ```conteo_palabras``` que contiene el conteo de cada palabra del *RDD* ```palabras_quijote```.\n",
    "\n",
    "* El método [```pyspark.RDD.map()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html) de la forma ```(<palabra>, 1)```.\n",
    "* El método [```pyspark.RDD.reduceByKey()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey.html) agrupará cada palabra y contará las veces que esta se repite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_palabras = palabras_quijote.map(lambda palabra: (palabra, 1))\\\n",
    ".reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conteo_palabras.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El método [```pyspark.RDD.sortBy()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html) regresará un *RDD* ordenado en forma descendente a partir del *RDD* ```conteo_palabras```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_ord = conteo_palabras.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_ord.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método [```pyspark.RDD.take()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.take.html) regresará un objeto de tipo ```list```de *Python* con los primeros 50 elementos del *RDD* ```conteo_ord```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_50 = conteo_ord.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ahora se realizarán todas las expresiones previas en una sola expresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_texto.flatMap(lambda x: x.split(\" \"))\\\n",
    "    .filter(lambda x: x != None)\\\n",
    "    .map(lambda palabra: (palabra, 1))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\\\n",
    "    .take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones de escritura de un *RDD*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [```saveAsHadoopDataset()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.saveAsHadoopDataset.html)\n",
    "* [```saveAsNewAPIHadoopDataset()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.saveAsNewAPIHadoopDataset.html)\n",
    "* [```saveAsPickleFile()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.saveAsPickleFile.html)\n",
    "* [```saveAsHadoopDataset()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.saveAsHadoopDataset.html)\n",
    "* [```saveAsHadoopFile()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.saveAsHadoopFile.html)\n",
    "* [```saveAsSequenceFile()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.saveAsSequenceFile.html)\n",
    "* [```saveAsTextFile()```](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.saveAsTextFile.html)\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./conteo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_ord.saveAsTextFile('conteo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./conteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_conteo = spark.sparkContext.textFile(\"./conteo/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_conteo.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licencia Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/80x15.png\" /></a><br />Esta obra está bajo una <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Licencia Creative Commons Atribución 4.0 Internacional</a>.</p>\n",
    "<p style=\"text-align: center\">&copy; José Luis Chiquete Valdivieso. 2022.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
