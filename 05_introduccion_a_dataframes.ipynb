{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![img/pythonista.png](img/pythonista.png)](https://www.pythonista.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a *Dataframes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Intro a Dataframes\").getOrCreate()\n",
    "ct = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *dataframes* son un concepto compartido entre plataformas como *R*, *Pandas* y *Scala*. Son estructuras tabulares en las que todos los datos de una columna comparten el mismo tipado. Cada columna tiene un título y cada renglón tiene un índice. A la descripción de los tipos de datos de cada columna de un *dataframe* se le conoce como esquema (*schema*).\n",
    "\n",
    "*PySPark* tiene la capacidad de poder manejar *dataframes* tanto de *Pandas* como de *Spark* e incluso cuenta con una [*API*](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) que optimiza la interacción entre ambos tipos de *dataframes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los *dataframes* de *Spark*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un *dataframe* de *Spark* es un objeto instanciado de la clase [```pyspark.sql.DataFrame```](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de *dataframes*.\n",
    "\n",
    "La función [```spark.createDataFrame()```](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html) permite crear *dataframes* a partir de objetos que se ingresan como argumentos.\n",
    "\n",
    "\n",
    "```pyspark\n",
    "df = spark.createDataFrame(data=<obj>, <títulos columnas>, schema=<esquema>)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "\n",
    "* ```<obj>``` es un objeto que represente una estructura tabular el cual puede ser:\n",
    "    * Una colección de *Python*.\n",
    "    * Un *dataframe* de *Pandas*.\n",
    "    * Un *RDD* de *Spark*.\n",
    "* ```<titulos columnas>``` es una lista de cadenas de caracteres que corresponden al título de cada columna.\n",
    "* ```<esquema>``` es una estructura de tipos de datos de *Spark* que describe el tipado de cada columna.\n",
    "    \n",
    "Cabe hacer notar que los *dataframes* de *Spark* se construyen de forma perezosa, por lo que aún cuando sean definidos, estos no serán creados hasta que sean requeridos.\n",
    "    \n",
    "Por convención se utiliza el nombre ```df``` para designar un dataframe genérico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los objetos ```Row ``` y ```Column```.\n",
    "\n",
    "Los *dataframes* de *Spark* están compuestos por 2 tipos de objetos.\n",
    "\n",
    "* Los objetos de tipo ```pyspark.sql.Row``` que representan a cada renglón del *dataframe*.\n",
    "* Los objetos de tipo ```pyspark.sql.Column``` que representan a cada columna del *dataframe*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferencias entre *RDD* y *Dataframes*.\n",
    "\n",
    "Los *RDD* de *Apache Spark*. Son colecciones de datos que pueden ser crudos (incluyendo binarios), semiestructurados (*JSON*, *XML*, *YAML*) o estructurados. Al final de cuentas los *RDD* funcionan de forma similar a un *data lake*, mientras que los *dataframes* siempre tiene una estructura de tabla.\n",
    "\n",
    "Los *RDD* son creados desde el el contexto de *Spark*, mientras que los dataframes son creados usando la *API* de *SQL*.\n",
    "https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El método ```df.show()```.\n",
    "\n",
    "El método ```df.show()``` muestra los primeros ```n``` números de un *dataframe* de *Spark*.\n",
    "\n",
    "```\n",
    "df.show(<n>)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "\n",
    "* ```<n>``` es el número de renglones desplegados. El valor por defecto es ```20```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplos:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las siguientes celdas crearán un *dataframe* de *Spark* a partir de un *dataframe* de *Pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se creará el *dataframe* de *Pandas* llamado ```pandas_df```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.DataFrame({'Dirección':('Sur', 'Norte', 'Sur', 'Este'),\n",
    "              'Rumbo':('Este', 'Noroeste', 'Norte', 'Norte'),\n",
    "             'Pasajeros':(12, 24, 32, 5),\n",
    "             'Documentado':(True, None, True, False) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se creará el *dataframe* de *Spark* a partir de ```pandas_df```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las siguientes celdas creará un *dataframe* de *Spark* a partir dde un *RDD* que contiene una colección de objetos ```Row```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará un *RDD* de *Spark* que contiene una sucesión de objetos tipo ```Row```.\n",
    "    * El objeto ```rdd``` tiene una estructura que puede ser convertida en una tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = ct.parallelize((Row('Sur', 'Este', '12', True),\n",
    "                     Row('Norte', 'Noroeste', '24', None),\n",
    "                     Row('Sur', 'Norte', '32', True),\n",
    "                     Row('Este', 'Norte', '5', False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_2 = ct.parallelize((Row('Sur', 'Este', '12', True),\n",
    "                     1, 2, 3, 4,\n",
    "                     Row('Sur', 'Norte', '32', True),\n",
    "                     Row('Este', 'Norte', '5', False),\n",
    "                      'uno', 'dos', b'121312412413523513462156725624567'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará un *dataset* a partir del *RDD* y además se ingresará como argumento el nombre de cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, \n",
    "                ['Dirección', 'Rumbo', 'Pasajeros', 'Documentado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará un *dataframe* al que se le asignarán los nombres de las columnas automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = spark.createDataFrame(rdd)\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de una columna de un *dataframe*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible acceder a una columna de un *dataframe* usando su nombre o usando su número de índice consecutivo iniciando desde ```0```. \n",
    "\n",
    "```\n",
    "df.<Nombre Columna>\n",
    "```\n",
    "\n",
    "```\n",
    "df[<n>]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Rumbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El atributo ```df.schema```.\n",
    "\n",
    "El atributo ```df.schema``` contiene la estructura del *schema* del *dataframe* como una instancia de la clase ```pyspark.sql.types.StructType``` que contiene una colección de instancias de tipo ```pyspark.sql.types.StructField```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El método ```df.printSchema()```.\n",
    "\n",
    "El método ```df.printSchema()``` despliega una cadena de caractéres describiendo el *schema* del *dataframe*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipado de *Spark*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Spark* cuenta con distintos tipos de datos que extienden a los tipo nativos de *Scala*, *Python* y *R*. El módulo ```pyspark.sql.types``` contiene a todas las clases correspondientes a dichos tipo.\n",
    "\n",
    "El siguente enlace apunta a la referncia de tipos de datos de *Spark*.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda importará todos los tipos de datos de *PySpark* al entorno de esta *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de *schemas* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *schemas* describen estructuras de tipos de datos en *Spark*. Estas estructuras no sólo describen tablas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El tipo ```pyspark.api.types.StructField```.\n",
    "\n",
    "El tipo [```pyspark.api.types.StructField```](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.types.StructField.html) permite definir esquemas tanto para describir las columnas de un *dataframe* como para describir *schemas* complejos como los que se pueden encontrar en *YAML* o *JSON*.\n",
    "\n",
    "```\n",
    "StructField(<identificador>, <tipo>, <nulificable>)\n",
    "```\n",
    "Donde:\n",
    "\n",
    "* ```<identificador>``` es el nombre del campo.\n",
    "* ```<tipo>``` es el tipo de dato del campo.\n",
    "* ```<nulificable>``` es un valor booleano que indica si el campo puede aceptar valores nulos.\n",
    "\n",
    "\n",
    "Las estructuras complejas que pueden crearse en los *schemas* son contenidas dentro de un objeto de tipo [```pyspark.api.types.StructType```](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.types.StructType.html).\n",
    "\n",
    "```\n",
    "StructType(<campo 1>, <campo 2>, ... , <campo n>)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "\n",
    "* ```<campo i>``` es un objeto de tipo ```StructField```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda describe un *schema* para el *dataframe* ```df```en el que la columna ```Pasajeros``` es un entero que va de ```-128```a ```127```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Dirección', StringType(), True), \n",
    "                     StructField('Rumbo', StringType(), True), \n",
    "                     StructField('Pasajeros', ByteType(), True), \n",
    "                     StructField('Documentado', BooleanType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda creará una *dataframe* a partir de ```pandas_df``` con el *schema* correspondiente a ```schema```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=pandas_df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y escritura de archivos para *dataframes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos hacia un *dataframe*.\n",
    "\n",
    "En el caso de *PySpark*  es posible leer y crear dataframes a partir de distintos formatos de archivo que describan estructuras tabulares. La propiedad [```spark.read```](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.read.html) contiene una familia de métodos que pueden importar y convertir en *dataframes* diversos tipos de documento y de diversas fuentes.\n",
    "\n",
    "```\n",
    "df = spark.read.<método>(<ruta>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escritura de datos desde un *dataframe*.\n",
    "\n",
    "En el caso de *PySpark*  es posible leer y crear dataframes a partir de distintos formatos de archivo que describan estructuras tabulares. Las siguientes propiedades de los *dataframes* permiten exportar dataframes a distintos formatos y destinos.\n",
    "\n",
    "* [```df.write```](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.write.html) perimte crear y escribir una serie de documentos en el fromato indicado.\n",
    "* [```df.writeTo```](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.writeTo.html) permite añadir (*append*) datos a un documento o estructura existente.\n",
    "* [```df.writeStream```](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.writeStream.html) permite enviar los datos del *dataframe* a un flujo.\n",
    "\n",
    "```\n",
    "df.write.<método>(<ruta>)\n",
    "```\n",
    "La propiedad ```df.write``` es una implementación del objeto [```DataFrameWriter```](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html#pyspark.sql.DataFrameWriter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Fuentes de datos compatibles para los *dataframes*.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources.html\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### El método ```spark.read.parquet()```.\n",
    " \n",
    " [*Apache Parquet*](https://parquet.apache.org/) es un formato binario capaz de almacenar estructuras de datos y conservar sus *schemas*.\n",
    " \n",
    " El siguiente enlace apunta a la documentación de la lectura/excritura de documentos en formato *parquet* para *Spark*.\n",
    " \n",
    " https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda importará los datos del archivo ```data/data_covid.parquet```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/data_covid.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El método ```df.toPandas()```.\n",
    "\n",
    "El método ```df.toPandas()``` permite crear un *dataframe* de *Pandas* a partir de un *dataframe* de *Apache Spark*.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuación se usará el método ```df.toPandas()``` para mostrar el *dataframe* ```df``` de mejor manera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Parquet es un formato bina**Ejemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### El método ```spark.read.csv()```.\n",
    " \n",
    " Los documentos *CSV* son documentos de texto en los que se ingresan datos usando un caracter separados. Por lo general, este separador es el caracter ```,```. Los documento CSV no conservan el tipo de dato y todos los elmentos son cadenas de caracteres. *Spark* puede leer este tipo de formatos, pero es necesario indicarl el *schema* que utilizará.\n",
    " \n",
    " El siguiente enlace apunta a la documentación de la lectura/escritura de documentos en formato *CSV* para *Spark*.\n",
    " \n",
    " https://spark.apache.org/docs/latest/sql-data-sources-csv.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplos:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda leerá los datos dentro del archivo [```data/data_covid.csv```](data/data_covid.csv). En este caso sólo se indica la opción de que existe un encabezado en el documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",\"true\").csv('data/data_covid.csv')\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda leerá los datos dentro del archivo [```data/data_covid.csv```](data/data_covid.csv). En este caso se indican las opciones de que existe un encabezado en el documento y que se inferirá el *schema* de las columnnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",\"true\").option(\"inferSchema\", \"true\").csv('data/data_covid.csv')\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La siguiente celda definirá un *schema* para el *dataframe* resultante de extraer el archivo *CSV*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('index', DateType(), True), StructField('AGUASCALIENTES', LongType(), True), StructField('BAJA CALIFORNIA', LongType(), True), StructField('BAJA CALIFORNIA SUR', LongType(), True), StructField('CAMPECHE', LongType(), True), StructField('CHIAPAS', LongType(), True), StructField('CHIHUAHUA', LongType(), True), StructField('DISTRITO FEDERAL', LongType(), True), StructField('COAHUILA', LongType(), True), StructField('COLIMA', LongType(), True), StructField('DURANGO', LongType(), True), StructField('GUANAJUATO', LongType(), True), StructField('GUERRERO', LongType(), True), StructField('HIDALGO', LongType(), True), StructField('JALISCO', LongType(), True), StructField('MEXICO', LongType(), True), StructField('MICHOACAN', LongType(), True), StructField('MORELOS', LongType(), True), StructField('NAYARIT', LongType(), True), StructField('NUEVO LEON', LongType(), True), StructField('OAXACA', LongType(), True), StructField('PUEBLA', LongType(), True), StructField('QUERETARO', LongType(), True), StructField('QUINTANA ROO', LongType(), True), StructField('SAN LUIS POTOSI', LongType(), True), StructField('SINALOA', LongType(), True), StructField('SONORA', LongType(), True), StructField('TABASCO', LongType(), True), StructField('TAMAULIPAS', LongType(), True), StructField('TLAXCALA', LongType(), True), StructField('VERACRUZ', LongType(), True), StructField('YUCATAN', LongType(), True), StructField('ZACATECAS', LongType(), True), StructField('Nacional', LongType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",\"true\").schema(schema).csv('data/data_covid.csv')\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las siguientes celda escribirán los datos del *dataframe* en diversos formatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('datos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('datos_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json('datos_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf datos_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option('encoding', 'UTF-8').json('datos_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nuevo = spark.read.option('encoding', 'UTF-8').json('datos_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de funciones de los *dataframes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index','AGUASCALIENTES').where(df.AGUASCALIENTES >1000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index','AGUASCALIENTES').where(df.index == date(2020,12,20)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index','AGUASCALIENTES').filter(df.index >= date(2020,12,1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index', 'AGUASCALIENTES').sort('AGUASCALIENTES').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index', 'AGUASCALIENTES').sort('AGUASCALIENTES', ascending=False).limit(12).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index', 'AGUASCALIENTES', 'Nacional').sort('AGUASCALIENTES', ascending=False).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('index', 'AGUASCALIENTES', 'Nacional').sort('Nacional', ascending=False).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_ags = df.select('index', 'AGUASCALIENTES').sort('AGUASCALIENTES', ascending=False).limit(10)\n",
    "dias_nacional = df.select('index', 'Nacional').sort('Nacional', ascending=False).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_ags.join(dias_nacional, 'index').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_ags.join(dias_nacional, 'index').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_ags.join(dias_nacional, 'index', how='full').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_ags.join(dias_nacional, 'index').select('index').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licencia Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/80x15.png\" /></a><br />Esta obra está bajo una <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Licencia Creative Commons Atribución 4.0 Internacional</a>.</p>\n",
    "<p style=\"text-align: center\">&copy; José Luis Chiquete Valdivieso. 2022.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
